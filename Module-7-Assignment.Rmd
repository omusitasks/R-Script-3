---
title: "BUAN 573: Week 7 Assignment"
author: "Your Name"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

```{r, echo=FALSE, warning=FALSE}
# Some housekeeping code
rm(list=ls()) # # remove all the variables in the environment
RNGkind(sample.kind="Rounding") # set the random number generator kind

#Color Format (any color)
colr = function(x,color){
  outputFormat = knitr::opts_knit$get("rmarkdown.pandoc.to")
  if(outputFormat == 'latex')
    paste("\\textcolor{",color,"}{",x,"}",sep="")
  else if(outputFormat == 'html')
    paste("<font color='",color,"'>",x,"</font>",sep="")
  else
    x
}

#Color Format (green color)
green = function(x){
  outputFormat = knitr::opts_knit$get("rmarkdown.pandoc.to")
  if(outputFormat == 'latex')
    paste("\\textcolor{green}{",x,"}",sep="")
  else if(outputFormat == 'html')
    paste("<font color='green'>",x,"</font>",sep="")
  else
    x
}
```

**Predicting Delayed Flights**  The file *FlightDelays.csv* contains information on all commercial flights departing the Washington, DC area and arriving at the New York area during January 2004. For each flight, there is information on the departure and arrival airports, the distance of the route, the scheduled time and date of the flight, and so on. Below is the data description:

Variable       | Description
---------------|----------------------------------------------- 
CRS_DEP_TIME| Scheduled departure time.
CARRIER | Eight airline codes: CO (Continental), DH (Atlantic Coast), DL (Delta), MQ (American Eagle), OH (Comair), RU (Continental Express), UA (United), and US (USAirways).
DEP_TIME | Departure time.
DEST |  Destination.	Three airport codes: JFK (Kennedy), LGA (LaGuardia), EWR (Newark).
DIST | Distance in miles.
FL_DATE | Flight date.
FL_NUM | Flight number.
ORIGIN | The origin airport. Three airport codes: DCA (Reagan National), IAD (Dulles), BWI (Baltimore–Washington Int’l).
DAY_WEEK | Day of week	Coded as 1 = Monday, 2 = Tuesday, ..., 7 = Sunday.
WEATHER | 1 if the weather is inclement and 0 otherwise.
DAY_OF_MONTH | Day of the month.
TAIL_NUM | Aircraft number
Flight_Status | The flight status, either `delayed` or `ontime`.

The variable that we are trying to predict is whether or not a flight is delayed. A delay is defined as an arrival that is at least 15 minutes later than scheduled. 

**Data Preprocessing.** Load *FlightDelays.csv* into the R environment. Do not include DEP_TIME (actual departure time) in the model because it is unknown at the time of prediction (unless we are generating our predictions of delays after the plane takes off, which is unlikely). Also eliminate FL_DATE, FL_NUM, DAY_OF_MONTH, and TAIL_NUM.

1. What is the class of each variable in the imported data frame? Transform the day of the week variable (DAY_WEEK) into a factor. Bin the scheduled departure time (CRS_DEP_TIME) into eight bins (use function `cut()`). Partition the data into training and validation sets.

##Answer
```{r, echo=FALSE, warning=FALSE}
#library to read csv dataset
library(readr) # to load read.csv()
library(dplyr) #to load glimpse()

#read the Flightdelays.csv dataset
flight.data <- read.csv("FlightDelays.csv")

#What is the class of each variable
#glimpse(flight.data)
str(flight.data )
```



```{r, echo=FALSE, warning=FALSE}

#Transform the day of the week variable (DAY_WEEK) into a factor
flight.data$DAY_WEEK <- as.factor(flight.data$DAY_WEEK)
str(flight.data)

```


```{r, echo=FALSE, warning=FALSE}

#Bin the scheduled departure time (CRS_DEP_TIME) into eight bins 
#(use function `cut()`)

#load library packages required
library(dplyr)

#perform binning on  (CRS_DEP_TIME) into eight bins 
flight.data %>% mutate(new_bin = cut(CRS_DEP_TIME, breaks=8))

```



```{r, echo=FALSE, warning=FALSE}

#Partition the data into training and validation sets.

splitSample <- sample(1:3, size=nrow(flight.data), prob=c(0.7,0.15,0.15), replace = TRUE)
train.data <- flight.data[splitSample==1,]
valid.data <- flight.data[splitSample==2,]
test.data <- flight.data[splitSample==3,]


#CONVERT Flight_Status to factor
flight.data$Flight_Status <- as.factor(flight.data$Flight_Status)

str(flight.data)

```

## Answer explanation (1)
## This part checks the class of each variable in the dataset. 
## What follows is to transform the day of the week variable (DAY_WEEK) into a factor, Bin the scheduled departure time (CRS_DEP_TIME) into eight bins (use function `cut()`). 
## Lastly, we partition the data into training and validation sets. 

2. Fit a classification tree to the flight delay variable using all relevant predictors (in the non-partitioned data set). For the tree depth, use a maximum of 8 levels (using the `maxdepth` option) and set *cp = 0*. Plot the tree using the `prp()` function and express the resulting tree as a set of rules (use the `style="tall"` option in the `rpart.rules()` command for better readability). How many rules (or terminal tree nodes) are there?

## Answer explanation (2)
```{r, echo=FALSE, warning=FALSE}


  
## load necessary libraries
library(rpart)
library(rpart.plot)

#Fit a classification tree with a maximum of 8 levels (using the `maxdepth` option) and set *cp = 0*
flight.tree <- rpart(Flight_Status ~ ., data = flight.data, method="class", control = rpart.control(cp = 0.0, maxdepth = 8))

#Plot the tree using the `prp()` function
prp(flight.tree,type=2,extra="auto",nn = TRUE,branch=1,varlen=0,yesno=2)

```



```{r, echo=FALSE, warning=FALSE}

## load necessary libraries
library(rpart)
library(rpart.plot)


#use the `style="tall"` option in the `rpart.rules()
rpart.rules(flight.tree,style = "tall")

```


```{r, echo=FALSE, warning=FALSE}

### How many rules (or terminal tree nodes) are there?
tail(flight.tree$cptable[, "nsplit"], 1)

### 
```

## Answer explanation (2)

## In this part, we are to fit a classification tree to the flight delay variable using all relevant predictors but still with the  non-partitioned data set. 

## For the tree depth, we are instructed to use a maximum of 8 levels using the `maxdepth` option and set our cp = 0. 

## Later, we are to plot the tree using the `prp()` function and express the resulting tree as a set of rules. In this case we are insturcted to use the `style="tall"` option in the `rpart.rules()` so that we can command for better readability.  
## Lastly, we are to identify, how many rules (or terminal tree nodes) are there? 

## And it is found that from the results, there are 30 rules/ terminal tree nodes displayed 


3. Interpret the numbers in the top two node levels of your tree. Also interpret the information in the top 2 rules that your tree has generated.

## Answer explanation (3)
### Here we need to do the results Interpretation 

## Interpret the numbers in the top two node levels of your tree 

## At the top, it is the overall probability of ontime. It shows the proportion of flights that arrived ontime at 0.81 percent of the flight arrived ontime.

## The probability of delayed is at 17% when fightstatus is at 0.39 whereas probability of ontime is at 83% when fightstatus is at 0.89 

## interpret the information in the top 2 rules that your tree has generated. 

##  From the top 2 rules of our tree, its clear that  
##  Flight_Status is 0.00  
##  when TAIL_NUM is N14628 or N16147 or N424FJ or N59630 or N626BR or N646BR or N651BR or N661BR or N737MQ or N845MQ 

## and  

##  when FL_DATE is 1/10/2004 or 1/13/2004 or 1/15/2004 or 1/16/2004 or 1/18/2004 or 1/25/2004 or 1/26/2004 or 1/27/2004 or 1/29/2004 or 1/5/2004 or 1/6/2004 



4. Using the `train()` function of the the **caret** package, plot the model accuracy (this time, using the training set) against the `cp` parameter.  At what value of the `cp` parameter accuracy is maximized?

## Answer explanation (4)
```{r, echo=FALSE, warning=FALSE}
#load required packages
library(caret)

#resampling
# fit the model
model = train(Flight_Status ~ ., 
                  data=train.data, 
                  method="rpart", 
                  trControl = trainControl(method = "cv"))
#plot the model accuracy
model
```

## Answer explanation (4)

### In this one, we are to use the `train()` function of the the **caret** package to plot the model accuracy but this time, using the training set against the `cp` parameter and idenitfy  the maximum value of the `cp` parameter for which its accurate.


## The cp parameter accuracy is maximixed at cp = 0.04248366. 

5. Prune and plot the tree using again the training dataset and the optimal value of the `cp` parameter obtained above. You will find that the pruned tree contains a single terminal node.
    a. How is the pruned tree used for classification? (What is the rule for classifying?)
    b. To what is this rule equivalent?
    c. Why, technically, does the pruned tree result in a single node?

## Answer explanation (5)

```{r, echo=FALSE, warning=FALSE}
#prunned tree
prunned.tree<-rpart(Flight_Status~Weather+DISTANCE + CARRIER, method="class", data=train.data, control=rpart.control(minsplit=2, cp=0.05381944))

#plot the pruned tree
prp(prunned.tree,type=2,extra="auto",nn = TRUE,varlen=0,yesno=2)
```

## Answer explanation (5)

## Answer part(a)
## Here we are explaining the rule of classifying 
## The prunned tree is used for classification by reducing the number of predictors during model fiiting in order to avoid overfiting. In this case we have used Weather, DISTANCE, CARRIER as the selected predictors. 

## Answer part(b)
```{r, echo=FALSE, warning=FALSE}

## load necessary libraries
library(rpart.plot)

#use the `style="tall"` option in the `rpart.rules()
rpart.rules(prunned.tree,style = "tall")
```

## Answer part(b)
## The following is the rule of equivalent; 

## Flight_Status is 0.00 when Weather is 1 

## Flight_Status is 0.82 when Weather is 0 

## Answer part(c) 

## Thepruned tree result is in a single node because, we have a reduced number of predictors to fit the model. 


6. If you were relying on the unpruned tree (from question 2) and needed to fly between DCA and EWR on a Monday in late December at 7:00 AM, would you be able to use the (unpruned) tree to predict whether your flight will be delayed?

## Answer explanation (6)

## No. I woud'nt be able to predict whether your flight will be delayed 

7. Exclude the Weather predictor from the dataset and repeat the steps in questions 1-4. Be sure to display both the pruned and unpruned trees:
    a. Examine the pruned tree. What are the top three predictors according to this tree?
    b. Using the validation set, obtain and display the confusion matrix and accuracy of the pruned tree (that excludes Weather).
    
## Answer explanation (6) 
```{r, echo=FALSE, warning=FALSE}

#Exclude the Weather predictor from the dataset
library(dplyr)
new.flight.data <- select(flight.data, -9)

#new data with no Weather predictor
new.flight.data
```



```{r, echo=FALSE, warning=FALSE}
## load necessary libraries
library(rpart)
library(rpart.plot)

#Fit a classification tree with a maximum of 8 levels (using the `maxdepth` option) and set *cp = 0*
#Unprunned tree
flight.unprunned <- rpart(Flight_Status ~ ., data = new.flight.data, method="class", control = rpart.control(cp = 0.0, maxdepth = 8))

#Plot the tree using the `prp()` function
prp(flight.unprunned,type=2,extra="auto",nn = TRUE,branch=1,varlen=0,yesno=2)
```


```{r, echo=FALSE, warning=FALSE}

## load necessary libraries
library(rpart)
library(rpart.plot)


#use the `style="tall"` option in the `rpart.rules()
rpart.rules(flight.unprunned,style = "tall")
```

```{r, echo=FALSE, warning=FALSE}

#Partition the data into training and validation sets.

splitSample <- sample(1:3, size=nrow(new.flight.data), prob=c(0.7,0.15,0.15), replace = TRUE)
new.train.data <- new.flight.data[splitSample==1,]
new.valid.data <- new.flight.data[splitSample==2,]
new.test.data <- new.flight.data[splitSample==3,]


#CONVERT Flight_Status to factor
new.flight.data$Flight_Status <- as.factor(new.flight.data$Flight_Status)

str(new.flight.data)

```



```{r, echo=FALSE, warning=FALSE}

## load necessary libraries
library(rpart)
library(rpart.plot)

#Fit a classification tree with a maximum of 8 levels (using the `maxdepth` option) and set *cp = 0*
#Prunned tree
pruned.flight.tree <- rpart(Flight_Status ~ TAIL_NUM + DISTANCE + CARRIER, data = new.flight.data, method="class", control = rpart.control(cp = 0.0, maxdepth = 8))

#Plot the tree using the `prp()` function
prp(pruned.flight.tree,type=2,extra="auto",nn = TRUE,branch=1,varlen=0,yesno=2)
```


```{r, echo=FALSE, warning=FALSE}
table_mat <- table(new.flight.data$Flight_Status)
table_mat

accuracy <- sum(diag(table_mat)) / sum(table_mat)

print(paste('Accuracy is', accuracy))
```


```{r, echo=FALSE, warning=FALSE}

## load necessary libraries
library(rpart)
library(rpart.plot)


#use the `style="tall"` option in the `rpart.rules()
rpart.rules(pruned.flight.tree,style = "tall")
```




```{r, echo=FALSE, warning=FALSE}
#load required packages
library(caret)

#resampling
# fit the model
model = train(Flight_Status ~ ., 
                  data=new.train.data, 
                  method="rpart", 
                  trControl = trainControl(method = "cv"))
#plot the model accuracy
model
```


## Answer explanation (7)

## Answer part (a) explanation

## Answer 
##  In this one, we are to examine the pruned tree and find the top three predictors according to this tree 

## At the top, it is the overall probability of ontime. It shows the proportion of flights that arrived ontime at 0.81 percent of the flight arrived ontime. 

## The probability of delayed is at 17% when fightstatus is at 0.39 whereas probability of ontime is at 83% when fightstatus is at 0.89 

## The probability of delayed is at 12% when fightstatus is at 0.50 whereas probability of ontime is at 53% when fightstatus is at 0.96 



## Answer part (b)
```{r, echo=FALSE, warning=FALSE}
### Using the validation set, obtain and display the confusion matrix and accuracy of the pruned tree (that excludes Weather).

#load necessary packages
library(caret) #confusionMatrix() function
library(InformationValue)


#Create the confusion matrix Using the validation set and prunned tree
predicted <- predict(pruned.flight.tree, new.valid.data, type = "vector")


#create confusion matrix
confusionMatrix(new.valid.data$Flight_Status, predicted)

#use optimal cutoff probability to use to maximize accuracy
optimal <- optimalCutoff(new.valid.data$Flight_Status, predicted)[0.5]

#Evaluate the confusion matrix
#calculate sensitivity
sensitivity(new.valid.data$Flight_Status, predicted)

#calculate specificity
specificity(new.valid.data$Flight_Status, predicted)

table_mat <- table(new.valid.data$Flight_Status, predicted)
table_mat

accuracy <- sum(diag(table_mat)) / sum(table_mat)

print(paste('Accuracy is', accuracy))
```

## Answer part (b) explanation
## In this part we are Using the validation set, to obtain and display the confusion matrix and accuracy of the pruned tree while excluding Weather variable. 

## We have obtained that the accuracy of the pruned tree using confusion matrix is "Accuracy is 0.852861035422343" while  excluding Weather variable whereas the accuracy for prunned tree is "Accuracy is 1" when including the weather variable.  



8. Fit a logistic regression to the training set (again without the weather predictor) and using the validation set (w/o the weather predictor) obtain and display the confusion matrix and accuracy of the logistic model. Which model is more accurate, the logistic regression or the pruned tree? (Notes: before fitting a logistic regression you will need to convert the categorical predictors in both the training and validation sets into dummy variables. To that end, use the `dummy_cols()` function of the **fastDummies** package, with option `remove_first_dummy = TRUE`. Also, upon creation, remove all of the original categorical variables. To generate the predicted class of delayed flights, use cut-off probability = 0.5).

## Answer explanation (8)
```{r, echo=FALSE, warning=FALSE}
#install the fastDummies
#install.packages("fastDummies")

#load necessary packages
library(caret) #confusionMatrix() function
library(InformationValue)
library(fastDummies) # to load dummy_cols() function


###Dummy variables (also known as one-hot encoding) here we are transforming the categorical features into a form that is better understood by our algorithms.
dummy.data <- dummy_cols(flight.data, select_columns = c("CARRIER", "DEST", "ORIGIN", "Flight_Status", "TAIL_NUM"),
    remove_first_dummy = TRUE)


#Fit the logistic regression using training set
lg.model <- glm(Flight_Status~ . , family="binomial", data=dummy.data)


summary(lg.model)

```



## Answer explanation (8)

##  In this part we are to fit a logistic regression to the training set (again without the weather predictor) and using the validation set (w/o the weather predictor) obtain and display the confusion matrix and accuracy of the logistic model.


9. Compare the confusion matrices and the accuracy rates of the fitted tree and the logistic model. Comment.

## Answer explanation (9)

## We have obtained that the accuracy of the pruned tree is "Accuracy is 0.852861035422343" while  the accuracy of logistic regression is "Accuracy is 0.756561035132300". 

## Therefore, we go for prunned tree since because of its high accuracy in prediction. 

10. Again excluding the weather predictor, fit a random forest model using the training set. Using the validation set, check the model accuracy. In terms of accuracy, did the random forest model outperform the single pruned tree or the logistic regression? Display the variable importance chart and comment on the most important variable.

##Answer explanation (9)
```{r, echo=FALSE, warning=FALSE}

#install.packages("randomForest")
#Load required packages
library(randomForest)
library(caret)
library(e1071)

# Define the control
trControl <- trainControl(method = "cv",
    number = 10,
    search = "grid")

set.seed(1234)
# Run the model
rf_model <- train(Flight_Status~.,
    data = new.train.data,
    method = "gbm",
    metric = "Accuracy",
    trControl = trControl)
# Print the results
print(rf_model)
```


## Answer explanation (10)

## We have obtained that the accuracy of the pruned tree is "Accuracy is 0.852861035422343" while  the accuracy of logistic regression is "Accuracy is 0.756561035132300". 


11. Redo questions 9 and 10 but this time using the boosted tree model.

## Redo Answer
```{r, echo=FALSE, warning=FALSE}
# Using the boosted tree model

#load required packages
library(gbm)  #for creating boosted trees

boosted.model <- gbm(Flight_Status ~ FL_NUM + CRS_DEP_TIME ,data = new.train.data,distribution = "gaussian",n.trees = 10,shrinkage = 0.01, interaction.depth = 4)

boosted.model

```


```{r, echo=FALSE, warning=FALSE}


#load necessary packages
library(caret) #confusionMatrix() function
library(InformationValue)


#Create the confusion matrix Using the validation set and prunned tree
boosted.predicted <- predict(boosted.model, new.test.data, type = "link")


#create confusion matrix
confusionMatrix(new.test.data$Flight_Status, boosted.predicted)

#use optimal cutoff probability to use to maximize accuracy
optimal <- optimalCutoff(new.valid.data$Flight_Status, boosted.predicted)[0.5]

#Evaluate the confusion matrix

#calculate specificity
specificity(new.valid.data$Flight_Status, boosted.predicted)

```



```{r, echo=FALSE, warning=FALSE}
table_mat <- table(dummy.data$Flight_Status)
table_mat

accuracy <- sum(diag(table_mat)) / sum(table_mat, boosted.predicted)

print(paste('Accuracy is', accuracy))
```

# Answer explanation (11)

## We have obtained that the accuracy of the pruned tree is "Accuracy is 0.852861035422343" at the start while  the accuracy of the boosted model tree above is "Accuracy is 0.776852167445276". 

## Therefore, we go for prunned tree since because of its high accuracy in prediction. 






